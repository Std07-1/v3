Нижче — **структурований “Host Contract / Capability Sheet”** для `vps-70511bd0`, який можна використовувати як SSOT для планування/деплою **v3** (і будь-яких наступних проєктів). Дані взяті з твоїх команд вище, без припущень там, де їх не можна довести.

---

## 0) Ідентифікація хоста

* **Hostname:** `vps-70511bd0`
* **Провайдер/платформа:** `OpenStack Nova` (vendor: OpenStack Foundation)
* **Віртуалізація:** `KVM (full)`
* **OS:** `Ubuntu 22.04.5 LTS (jammy)`
* **Kernel:** `5.15.0-164-generic`
* **Uptime:** `52 дні`
* **Поточне навантаження:** `loadavg ~ 0.03 / 0.01 / 0.00` (фактично idle)

**Висновок:** це стандартна VM у OpenStack/KVM, без контейнеризації на рівні ОС.

---

## 1) CPU (Compute envelope)

### Факти

* **vCPU:** `4`
* **Модель:** `Intel Core Processor (Haswell, no TSX)` (гіпервізор KVM)
* **Threads/core:** `1`
* **NUMA:** `1 node`
* **В top:** CPU idle ~ `98%`

### Практичні наслідки для v3

* Це **невеликий compute** (4 vCPU). Будь-які heavy-процеси (масова агрегація/індикатори/бекфілл на багато символів/TF) треба:

  * або **різко бюджетувати**,
  * або виносити в окремий воркер/окремий хост,
  * або робити “collapse-to-latest” + single-inflight per (symbol, tf) (у твоєму стилі rail-ів).

---

## 2) RAM / Swap (Memory envelope)

### Факти

* **RAM total:** `~7.6 GiB`
* **Swap:** `0` (відсутній)
* **MemAvailable:** `~7.0 GiB` (зараз вільно багато)

### Практичні наслідки для v3

* Відсутність swap = **OOM буде різкішим** при піках пам’яті (падіння процесу, не “підвисання”).
* Дизайн v3 має уникати:

  * великих in-memory буферів без caps,
  * необмежених черг,
  * “історія в RAM” без TTL/лімітів.

Рекомендація як “policy”: **будь-який кеш/черга → max_items/max_bytes + TTL**.

---

## 3) Диск / FS (Storage envelope)

### Факти

* **Disk:** `sda ~75G (QEMU HARDDISK)`
* **Root FS:** `ext4`, розмір `~73G`, зайнято `~5.1G (8%)`
* **Inodes:** ок (використано ~2%)
* **ROTA=1** (для vdisk інколи показує “rotational”, але по суті це “віртуальний диск”; реальна продуктивність залежить від backing storage провайдера)

### Практичні наслідки для v3

* Місця багато, але **IO latency/throughput невідомі** (типово це головний bottleneck на VPS).
* Якщо v3 буде зберігати медіа/бекапи/історію:

  * **лог-ротація обов’язкова**
  * write amplification мінімізувати
  * великі файли — краще об’єктне сховище (S3/R2), а не root-диск, якщо це “назавжди”.

---

## 4) Мережа (Network envelope)

### Факти

* Інтерфейс: `ens3`
* **IPv4:** `162.19.152.83/32` (так, /32 — нормально в OpenStack)
* **IPv6:** `2001:41d0:701:1100::caed/128`
* Default GW: `162.19.152.1`
* DNS: `213.186.33.99`
* `ss -s`: всього `174` сокети, TCP `11`, estab `2` — дуже мало (тобто мережа не “забита” зараз)

### Практичні наслідки для v3

* Мережа не виглядає проблемною.
* Якщо лишаєш Cloudflare попереду — основні ризики не “bandwidth”, а:

  * **timeouts між Cloudflare ↔ origin**
  * rate-limit / conn-limit
  * реальний IP (realip) та журнали.

---

## 5) Відкриті порти / доступність сервісів (Exposure surface)

### Факти (по `ss -lntp`)

* `nginx` слухає **0.0.0.0:80,443** і **[::]:80,443**
* `redis-server` слухає **127.0.0.1:6379** і **[::1]:6379** (тобто *не* доступний ззовні)
* `sshd` слухає `:22`

### Практичні наслідки для v3

* Зовнішня поверхня атаки: **22/80/443**.
* Redis вже правильно ізольований на loopback.

---

## 6) UFW / Firewall модель (Security perimeter)

### Факти (з твого попереднього логу UFW)

* **22/tcp** дозволено **Anywhere** (SSH відкритий у світ)
* **80/443** дозволені **лише з Cloudflare IP ranges** (v4 + v6)

### Що це означає “людською мовою”

* Веб (80/443) **майже повністю “закритий від прямого інтернету”** і доступний лише через Cloudflare (бо тільки Cloudflare IP можуть достукатися до nginx на origin).
* Це **не діра**, а навпаки **зменшення** поверхні атаки на origin.
* Ризик тут інший: якщо ти помилково:

  * довіриш real IP не тільки Cloudflare,
  * або відкриєш 80/443 “Anywhere”,
  * тоді origin стає напряму доступним і сканери починають валити в nginx.

### Для v3

* Така ж схема працюватиме з іншими проєктами **без проблем**, якщо вони теж сидять за nginx (reverse-proxy) і Cloudflare.

---

## 7) Kernel/системні ліміти (sysctl)

### Факти

* `net.core.somaxconn = 4096` (добре)
* `net.netfilter.nf_conntrack_max = 262144` (нормально)
* `net.ipv4.ip_local_port_range = 32768..60999` (стандарт)
* `vm.max_map_count = 65530` (стандарт)
* `fs.nr_open = 1048576` (високо)
* `fs.file-max = 9e18…` (фактично “не ліміт” на рівні ядра)

### Практичні наслідки для v3

* На рівні ядра все ок.
* Але **реальний стопер у тебе не ядро, а `ulimit -n`** (див. далі).

---

## 8) User-space ліміти (ulimit) — критичний пункт

### Факти (`ulimit -a`)

* **open files (-n): 1024**  ← *низько*
* max user processes (-u): 30825
* stack 8192 KB (стандарт)

### Чому це важливо

1024 FD — це легко пробити навіть “невинною” системою:

* nginx (коннекти, файли, логи),
* WebSocket,
* Python runtime (сокети, файли, DNS),
* плюс системні бібліотеки.

**Це одна з типових причин “дивних” таймаутів/522/нестабільності під навантаженням.**

### Дія для v3 (як policy)

* Для **nginx** і для **v3.service** виставляти `LimitNOFILE=65535` (або хоча б 16384).
* Для shell `ulimit` не покладатися — важливі саме systemd limits в unit-файлах.

(Якщо скажеш назву майбутнього сервісу v3 — дам готовий unit-шаблон.)

---

## 9) Поточний runtime-профіль (що вже “є” на хості)

### Факти

* `nginx` активний
* `redis` активний, локальний
* `systemd-resolved` активний
* `smc.service` зараз зупинений (ти його паузив раніше)

### Практичні наслідки для v3

* Є готовий фундамент: nginx + systemd + локальний redis.
* Логічно “вписати” v3 у ту саму модель:

  * **v3 API** на `127.0.0.1:<порт>`
  * nginx reverse-proxy на домені/шляху
  * Cloudflare перед ним (якщо треба)
  * UFW: 80/443 тільки з Cloudflare

---

## 10) “Host Contract” для асистентів (короткий машинний конспект)

**Compute**

* vCPU=4, RAM=7.6GiB, swap=0

**Storage**

* root ext4 ~75G vdisk (IO характеристики невідомі, припускати “середні/слабкі”)

**Network**

* IPv4 /32, IPv6 /128, DNS 213.186.33.99
* inbound 80/443 тільки Cloudflare (за UFW політикою)
* ssh 22 open to world

**Runtime**

* nginx: 80/443 public
* redis: loopback only
* systemd for services

**Hard limits**

* ulimit -n = 1024 (потрібно підняти у systemd units для nginx та app)

---

## 11) щоб “контракт” був 100% повний

1. Ліміти systemd для nginx (вони можуть відрізнятись від `ulimit` у shell):

MemoryMax=infinity
TasksMax=9247
LimitNOFILE=524288

---
