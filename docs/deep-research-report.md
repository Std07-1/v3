# TradingView-подібна швидкість графіка: range-based догрузка без «рваного» рендеру

## Контекст і коректна постановка проблеми

Ви будуєте read-only веб‑UI для перегляду OHLCV‑графіків по ~13 інструментах і кількох TF, з подальшим підключенням SMC‑оверлеїв. Поточний стек уже має правильні базові елементи: SSOT на диску (JSONL), HTTP API для барів і інкрементальних оновлень, а також lightweight‑рендер на основі Lightweight Charts (через `chart_adapter_lite`). fileciteturn32file0

На клієнті реалізовано:

- cold‑load “останні N барів” (`limit≈20000`) для (symbol, TF);
- polling оновлень через `/api/updates`;
- догрузку історії «вліво» чанками (типово ~1000) при наближенні до лівого краю видимого діапазону;
- фоновий prefetch (щоб перемикання symbol/TF було майже миттєвим). fileciteturn28file0turn32file0

Проблема проявляється як UX‑дефекти при навігації:

- під час швидкого скролу вліво догрузка не встигає за діями користувача: зʼявляються «дірки», візуальні стрибки та відчуття «рваного» графіка;
- при zoom‑out (стисканні барів) у вʼюпорті стає потрібно більше історії, ніж є в клієнтському кеші/масиві, але автоматична догрузка під новий видимий діапазон спрацьовує погано або не спрацьовує взагалі. fileciteturn28file0turn29file0

Важливий системний нюанс: ви свідомо не хочете тримати «все й одразу» в UI‑памʼяті, бо (а) трейдер реально працює з 1–2 інструментами у моменті, (б) далі додається SMC‑шар, який сам потребуватиме памʼяті та обчислювального бюджету, (в) потрібні передбачувані межі споживання RAM/CPU на клієнті. fileciteturn30file1turn32file0

Це формулюється для спеціалістів так: **потрібен детермінований range‑based loader**, який:

1) тримає покриття даними під поточний `visible range` + запас (overscan),  
2) реагує і на pan/scroll, і на zoom‑out/zoom‑in,  
3) не породжує «перемальовувань із розривами» (jank/flicker) і race‑condition між scroll ↔ fetch ↔ `setData`,  
4) використовує контроль конкурентності (single‑inflight, abort, latest‑wins),  
5) має багаторівневий кеш (hot/warm/cold) і не ламає майбутній SMC‑pipeline. fileciteturn30file1turn28file0

## Як TradingView робить «плавний» range-based UX

Якщо говорити про механіку, а не «магію», то ключ у тому, що в TradingView (у їхній Advanced/Charting Library екосистемі) **догрузка привʼязана не до “події скролу як такої”, а до потреби заповнити візуальний простір**.

У Datafeed API бібліотека викликає `getBars`, коли їй потрібен фрагмент історії **для конкретного діапазону**. У параметрах приходять:

- `from` і `to` як часовий інтервал **`[from, to)`**;
- `countBack` — **точна кількість барів**, які потрібні, щоб заповнити простір графіка (це пріоритетніше за точне попадання в `from/to` в типових кейсах);
- `firstDataRequest` — ознака першого запиту. citeturn0search0turn0search6

Критично: документація прямо каже, що **бібліотека сама рахує, скільки барів треба “щоб заповнити графік”**, і цей обсяг ви не можете «перевизначити» зі сторони datafeed. citeturn0search0

Якщо ви віддаєте менше даних, ніж потрібно (наприклад, через фільтр сесій/календар або прогалини в бекенді), бібліотека може викликати `getBars` повторно, намагаючись «дотягнути» відсутні бари. Це нормальна поведінка, і саме тому TV‑підхід не виглядає «дірявим» — доки datafeed коректно догружає потрібний `countBack`. citeturn0search5turn0search0

Ще одна важлива деталь: **історичні дані кешуються на стороні бібліотеки**, тобто вам не потрібно вигадувати клієнтський кеш “щоб перемикання було швидким” — він уже частина механізму (для Advanced/Charting Library). citeturn0search0

Для realtime TradingView розділяє семантику: `subscribeBars` дозволяє оновлювати тільки останній бар або додавати новий; спроба “переписати історичний бар” веде до типових помилок (“time violation”) і має робитися через reset‑механізми. citeturn0search2

Нарешті, навіть у reset‑сценаріях у них є спеціальна оптимізація: featureset `request_only_visible_range_on_reset` дозволяє при `resetData` перезапитувати лише видимий діапазон замість “всього вже завантаженого”. Це ще одне підтвердження, що TV‑логіка мислить категоріями **видимого діапазону та потрібного покриття**, а не категоріями “тягнемо великий ліміт і живемо з ним”. citeturn1search0turn1search2

Практичний висновок для вашого UI: **TV‑like UX = loader, що мислить “покриттям під visible range”, і має внутрішню кеш‑модель та алгоритм догрузки на zoom/scroll**. citeturn0search0turn1search0

## Що дає Lightweight Charts і звідки береться «рваний» рендер

Lightweight Charts — це рендерер + базова взаємодія, але **data‑management (кеші, дозапити, stitching) — ваша задача**. Саме тут зазвичай і народжується «рваність», якщо loader привʼязаний до неправильного сигналу або занадто часто робить важкі операції.

Два найбільш “неспростовні” обмеження/рекомендації з документації:

- `setData` замінює весь масив даних серії та може суттєво бити по продуктивності; для realtime рекомендується `update`, а не `setData`. citeturn0search9  
- для імплементації догрузки історії під час скролу в документації прямо рекомендується звʼязка `subscribeVisibleLogicalRangeChange` + `series.barsInLogicalRange(...)`, щоб **не допустити ситуації, коли користувач бачить порожній простір**. citeturn2search2turn2search0

Ключова відмінність: **`visible logical range` — це індекси в серії**, а не час. І саме `barsInLogicalRange` повертає метадані, які описують, скільки барів є “до” та “після” поточного діапазону (`barsBefore`, `barsAfter`). Семантика важлива:

- `barsBefore > 0` означає, що “зліва ще є бари поза діапазоном” (тобто запас є),
- `barsBefore < 0` означає, що **перший бар серії вже потрапляє всередину requested range** і між ним та лівою межею діапазону є “порожній логічний простір” (класичний індикатор, що користувач може побачити пустоту і треба догружати). citeturn2search0

Окрема пастка, яка напряму пояснює ваш кейс з zoom‑out: метод `timeScale.getVisibleRange()` (visible time range) **не вміє екстраполювати час за межі вже наявних data points**, тому може “підрізати” діапазон до наявних даних. Документація прямо каже: щоб отримати “повну інформацію” про видимий діапазон, потрібно використовувати `getVisibleLogicalRange` і `ISeriesApi.barsInLogicalRange`. citeturn1search10turn2search2

Як це бʼється з вашим станом коду:

- у вас є підписка на зміну visible logical range на рівні адаптера (це правильно) fileciteturn29file0,
- але в клієнтському loader‑і тригер «коли догружати» наразі зашитий як поріг по `range.from` (умовно “якщо `from < 80` — догружати”) та ще й містить блокуючі умови, які можуть зупиняти догрузку саме в режимі “я на правому краї”. fileciteturn28file0  
Це більш крихко, ніж перевірка через `barsInLogicalRange`, бо `range.from` сам по собі не каже, чи бачить користувач пустоту і чи вистачає буфера при zoom‑out.

Додатковий важіль (не про логіку тригерів, а про продуктивність при zoom‑out): у Lightweight Charts v5.1 додали **data conflation** — автоматичне “згортання” точок при сильному zoom‑out, щоб покращити рендер великих датасетів. Фіча opt‑in через `enableConflation` в опціях time scale/series. citeturn2search4turn2search11  
Це не замінює range‑based loading, але може зняти частину “не встигає перемальовуватись” саме при дуже стислому вигляді.

## Архітектура range-based догрузки під ваш стек

Нижче — модель, яка максимально близька до TV‑логіки, але в реаліях Lightweight Charts і вашого SSOT/HTTP API.

### Контракт даних і API

У вас `/api/bars` уже підтримує параметри `since_open_ms` та `to_open_ms` + `limit`, а також `force_disk`/`prefer_redis`. Це вже фактично “cursor‑based” API (запит “до часу X” та “після часу Y”). fileciteturn30file0turn32file0

Щоб loader був простішим і менш крихким, варто формально зафіксувати один із двох контрактів (обидва робочі):

- **Cursor‑based (як зараз, але чітко названий):**  
  - `GET /api/bars?symbol&tf_s&before_open_ms=<t>&limit=<n>` → “n барів строго раніше t”  
  - `GET /api/bars?symbol&tf_s&after_open_ms=<t>&limit=<n>` → “n барів строго пізніше t”  
  Такий контракт психологічно простіший, ніж `to_open_ms = firstOpenMs-1`, і зменшує шанс off‑by‑one.

- **TradingView‑подібний (UDF‑like):**  
  - `GET /history?symbol&resolution&to=<unix>&countback=<n>` (або `from/to`)  
  Тут `countback` має пріоритет над `from` — саме так це описано в UDF/Charting Library документації, і це природно лягає на задачу “заповнити видимий простір + запас”. citeturn0search7turn0search0

Якщо є шанс, що ви колись захочете мігрувати на Advanced/Charting Library, UDF‑подібний контракт — найменш болючий шлях. Якщо ні — cursor‑based цілком достатній.

### Ключова логіка: керування “покриттям” видимого діапазону

Правильна точка входу — не “скрол подія”, а **зміна видимого логічного діапазону** (включає і pan, і zoom). У вас ця інфраструктура вже є: адаптер підписується на `subscribeVisibleLogicalRangeChange`/`subscribeVisibleTimeRangeChange` і прокидує callback у клієнт. fileciteturn29file0turn28file0

Але тригер має оцінювати не лише `range.from`, а **реальну нестачу даних**. І саме для цього в Lightweight Charts існує `barsInLogicalRange`, який документація наводить як canonical‑приклад для “download history while scrolling to prevent empty space”. citeturn2search2turn2search0

Рекомендована заміна вашого “порога по `range.from`”:

1) На кожну зміну `visible logical range`:

- отримати `barsInfo = mainSeries.barsInLogicalRange(range)`
- якщо `barsInfo === null` → нічого не робити (нема даних)
- якщо `barsInfo.barsBefore < prefetchBarsLeft` → ініціювати догрузку зліва
- якщо треба (рідше у вашому кейсі, бо realtime йде через `/api/updates`), аналогічно по `barsAfter` справа citeturn2search2turn2search0

1) Догрузка зліва:

- single‑inflight на (symbol, tf) + throttle (мін. інтервал) — у вас уже є базові елементи цього підходу fileciteturn28file0
- chunk size робити **адаптивним**, а не фіксованим:
  - мінімум: 500–2000 барів (залежно від TF),
  - при zoom‑out (коли `barsBefore` стає відʼємним або дуже малим) chunk має зростати, бо інакше користувач “розкриває” range швидше, ніж ви додаєте дані.

1) Stitching без стрибків:

- перед prepend зберегти `prevVisible = getVisibleLogicalRange()`
- після `setData(mergedBars)` зробити `setVisibleLogicalRange({from: prev.from + addedCount, to: prev.to + addedCount})`  
Це рівно той механізм, який ви вже описували як задум для TV‑подібної поведінки. fileciteturn30file1turn28file0turn29file0

1) Критичний момент для вашого кейсу zoom‑out: **не блокуйте догрузку лише тому, що ви “на правому краї”**. У TradingView‑подібній логіці zoom‑out на правому краї має право розширити видимий діапазон вліво і вимагати історії (тобто це не “скрол вліво”, але це все ще “потрібно заповнити viewport”). Це випливає з того, що Charting Library рахує потрібний `countBack` саме щоб “заповнити chart space” і буде викликати `getBars` незалежно від того, чи користувач був на last bar. citeturn0search0turn0search6  
У Lightweight Charts це найкраще ловиться через `barsBefore`/`barsInLogicalRange`, а не через “я на ендпоінті”. citeturn2search2turn2search0

### Чому це прибирає «дірки» і «рваність»

- Ви починаєте догрузку **до того**, як користувач побачить пустий простір: саме так задуманий `barsInLogicalRange`. citeturn2search2turn2search0  
- Ви зменшуєте кількість дорогих `setData` (менше дрібних чанків → менше повних перемальовувань), узгоджено з рекомендацією “не викликати `setData` часто”. citeturn0search9  
- Ви робите корекцію visible range після prepend, щоб viewport не “стрибав”, що прямо відповідає вашій внутрішній нотатці про стабільність logical range після додавання старих барів. fileciteturn30file1turn29file0

## Кешування і керування конкурентністю

### Що у вас вже добре

- На бекенді `/api/bars` має Redis‑cold‑load із fallback на диск, і `/api/updates` читає tail‑only з SSOT. Це правильний напрям: на старті UI отримує “теплий” зріз швидко, а інкрементальні апдейти не вимагають дорогих повних сканів. fileciteturn32file0turn30file0  
- На фронті є `AbortController` для load‑запитів і окремий abort для scrollback — це базовий контроль конкурентності. fileciteturn28file0

### Де варто підтягнути до «TV‑рівня»

1) **Єдина політика single‑inflight per (symbol, tf) для кожного типу запитів** (initial load, scrollback left, optional forward fill).  
У вас це майже є, але TV‑like UX вимагає ще “coalescing”: якщо прийшло кілька тригерів на догрузку, доки in‑flight, не стартувати нові — а запамʼятати “найбільш потрібну” ціль (найменший `before_open_ms`) і виконати її після завершення поточного. Це знижує дерганину й трафік.

2) **Поріг + гістерезис у термінах `barsBefore`, а не у термінах `range.from`.**  
Документація Lightweight Charts прямо показує такий патерн: “якщо `barsBefore < 50`, тоді догружай”. Це більш стабільно при zoom‑out, при зміні barSpacing і при будь‑яких “особливостях” logical range. citeturn2search2turn2search0

3) **Окреме обмеження на клієнтський датасет (`MAX_RENDER_BARS`) — це не лише про памʼять, а й про UX.**  
Зараз `MAX_RENDER_BARS=20000`. fileciteturn28file0  
Якщо ви хочете TV‑подібну поведінку “zoom‑out показує більше історії” на тому ж TF, вам треба або піднімати цей ліміт (хоча б для активного інструмента), або вводити LOD‑стратегію: при дуже великому zoom‑out автоматично переходити на грубіший TF (15m/1h/4h), поки користувач не zoom‑in. Це концептуально схоже на те, як Charting Library працює з “потрібним обсягом барів для заповнення простору” і підтримкою різних resolutions у datafeed. citeturn0search0turn0search8

4) **LRU‑кеш UI не має бути “всі символи × поточний TF × 20000”.**  
У вас є prefetch для всіх символів на вибраному TF. fileciteturn28file0  
Щоб не готувати проблему для SMC‑шару, краще формалізувати:

- hot: активний (symbol, tf) — з максимальною глибиною (саме тут потрібен “TV feel”);
- warm: last‑K комбінацій (symbol, tf) з обмеженим `MAX_RENDER_BARS` (напр. K=3…5);
- cold: диск/Redis як SSOT. fileciteturn30file1turn32file0  
Саме так ваші research‑нотатки вже формулюють hot/warm/cold модель. fileciteturn30file1

1) **Оптимізація рендеру на zoom‑out: розглянути оновлення Lightweight Charts до v5.1+ і ввімкнення `enableConflation`.**  
Це не панацея, але це офіційно заявлена оптимізація для “десятків тисяч точок і zoom‑out”, і вона може прибрати частину “не встигає перемальовуватись” навіть тоді, коли loader уже правильний. citeturn2search4turn2search11

## Заділ під SMC-шар без вибуху памʼяті й перерахунків

SMC‑шар майже неминуче вводить два додаткові навантаження: (1) зберігання похідних структур (зони/ліквідність/свінги), (2) перерахунки при догрузці історії. Якщо UI‑loader залишиться “дірявим”, SMC стане або неправильним (бо window неповний), або дуже дорогим (бо доведеться постійно перевираховувати великі діапазони).

Тут корисно мислити так само, як мислить TradingView у Datafeed API для marks: вони окремо запитують “мітки” для видимого діапазону (`getMarks` викликається для visible bars range, і час міток має співпадати з часом барів). Це прямий патерн “похідні дані повинні бути range‑aware”. citeturn0search1turn0search2

Практична проєкція на вашу архітектуру:

- **SMC‑обчислення мають бути відокремлені від UI‑потоку** (SoC): UI тільки рендерить і відправляє запит “дай SMC‑оверлеї для [from,to] / для цього чанка”. fileciteturn30file1turn32file0  
- **Ключування строго по (symbol, tf, open_ms)** — це той самий інваріант, який ви вже використовуєте для барів/івентів. fileciteturn32file0turn30file0  
- **Після prepend старих барів**: SMC‑пайплайн рахує лише на новому префіксі (incremental backfill), а не перераховує все з нуля. Це можливо тільки якщо bar‑store гарантовано відсортований, дедупнутий і без “дір” у межах очікуваної календарної геометрії. fileciteturn30file1turn32file0

Так ви збережете SSOT‑принцип і не перетворите UI на “God‑module”, який одночасно тягне дані, стикує чанки, рахує SMC і ще й воює з продуктивністю.

Головний системний висновок: **першим шаром має бути правильний range‑based loader на бари**, який тригериться від visible range змін і перевіряє нестачу даних через `barsInLogicalRange`‑подібну метрику. Після цього SMC‑шар стає керованим (бо отримує стабільний window), а оптимізації кешів і concurrency перестають бути “латанням дір”, і стають прогнозованими інженерними рішеннями. citeturn2search2turn0search0turn0search9
