"""
tools/mcp/platform_server.py ‚Äî MCP —Å–µ—Ä–≤–µ—Ä –¥–ª—è Claude Copilot.

–ù–∞–¥–∞—î Claude —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∏–π –¥–æ—Å—Ç—É–ø –¥–æ runtime —Å—Ç–∞–Ω—É Trading Platform v3:
- HTTP API –ø–ª–∞—Ç—Ñ–æ—Ä–º–∏ (status, bars, updates, config)
- Redis live state
- JSONL data files –Ω–∞ –¥–∏—Å–∫—É
- Exit gates
- Log-—Ñ–∞–π–ª–∏

–ó–∞–ø—É—Å–∫: Python 3.13+, mcp SDK ‚â•1.0
  python tools/mcp/platform_server.py

–ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è: .vscode/mcp.json (auto-detected by VS Code Copilot)
"""

from __future__ import annotations

import datetime
import json
import os
import subprocess
import sys
import time
from pathlib import Path

from mcp.server.fastmcp import FastMCP

# ‚îÄ‚îÄ Resolve project root ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
_THIS_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = _THIS_DIR.parent.parent
CONFIG_PATH = PROJECT_ROOT / "config.json"
DATA_ROOT = PROJECT_ROOT / "data_v3"
LOGS_DIR = PROJECT_ROOT / "logs"

# ‚îÄ‚îÄ Platform API defaults ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
API_BASE_V3 = "http://127.0.0.1:8089"
WS_BASE_V4 = "http://127.0.0.1:8000"

# ‚îÄ‚îÄ TF labels ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TF_LABELS = {
    60: "M1", 180: "M3", 300: "M5", 900: "M15",
    1800: "M30", 3600: "H1", 14400: "H4", 86400: "D1",
}
LABEL_TO_TF = {v: k for k, v in TF_LABELS.items()}

# ‚îÄ‚îÄ MCP server ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
mcp = FastMCP("aione-trading-platform")


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# HELPERS
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def _http_get(url: str, timeout: float = 5.0) -> dict:
    """GET –∑–∞–ø–∏—Ç –¥–æ HTTP API –ø–ª–∞—Ç—Ñ–æ—Ä–º–∏. –ü–æ–≤–µ—Ä—Ç–∞—î parsed JSON."""
    import urllib.request
    import urllib.error
    try:
        req = urllib.request.Request(url)
        with urllib.request.urlopen(req, timeout=timeout) as resp:
            return json.loads(resp.read().decode("utf-8"))
    except urllib.error.URLError as e:
        return {"error": f"–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ ({url}): {e.reason}"}
    except Exception as e:
        return {"error": f"HTTP –ø–æ–º–∏–ª–∫–∞: {e}"}


def _redis_client():
    """–û—Ç—Ä–∏–º–∞—Ç–∏ Redis client. –ü–æ–≤–µ—Ä—Ç–∞—î None —è–∫—â–æ Redis –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π."""
    try:
        import redis
    except ImportError:
        return None
    try:
        cfg = json.loads(CONFIG_PATH.read_text(encoding="utf-8"))
        rcfg = cfg.get("redis", {})
        if not rcfg.get("enabled", False):
            return None
        return redis.Redis(
            host=rcfg.get("host", "127.0.0.1"),
            port=rcfg.get("port", 6379),
            db=rcfg.get("db", 1),
            decode_responses=True,
            socket_timeout=3,
        )
    except Exception:
        return None


def _load_config() -> dict:
    """–ü—Ä–æ—á–∏—Ç–∞—Ç–∏ config.json."""
    try:
        return json.loads(CONFIG_PATH.read_text(encoding="utf-8"))
    except Exception as e:
        return {"error": str(e)}


def _sym_dir(symbol: str) -> str:
    """–ö–∞–Ω–æ–Ω–∏—á–Ω–µ —ñ–º'—è –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó —Å–∏–º–≤–æ–ª—É: XAU/USD ‚Üí XAU_USD."""
    return symbol.replace("/", "_")


def _format_ms(ms: int | float) -> str:
    """Epoch ms ‚Üí ISO8601 UTC —Ä—è–¥–æ–∫."""
    try:
        dt = datetime.datetime.utcfromtimestamp(ms / 1000)
        return dt.strftime("%Y-%m-%d %H:%M:%S UTC")
    except Exception:
        return str(ms)


def _tf_resolve(tf_input: str) -> int | None:
    """–†–æ–∑–ø–∞—Ä—Å–∏—Ç–∏ TF –∑ label –∞–±–æ —á–∏—Å–ª–∞: 'M5' ‚Üí 300, '300' ‚Üí 300."""
    if tf_input in LABEL_TO_TF:
        return LABEL_TO_TF[tf_input]
    try:
        v = int(tf_input)
        if v in TF_LABELS:
            return v
        return None
    except ValueError:
        return None


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# TOOL 1: Platform Status
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@mcp.tool()
def platform_status() -> str:
    """
    –û—Ç—Ä–∏–º–∞—Ç–∏ –ø–æ–≤–Ω–∏–π —Å—Ç–∞—Ç—É—Å Trading Platform v3.
    –ü–æ–∫–∞–∑—É—î: boot_id, prime_ready, redis —Å—Ç–∞–Ω, preview/nomix,
    disk policy telemetry, bar counts per TF, uptime.

    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π —Ü–µ–π —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –ü–ï–†–®–ò–ú –ø—Ä–∏ –±—É–¥—å-—è–∫–æ–º—É –∞–Ω–∞–ª—ñ–∑—ñ
    —Å—Ç–∞–Ω—É —Å–∏—Å—Ç–µ–º–∏ –∞–±–æ –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏—Ü—ñ –ø—Ä–æ–±–ª–µ–º.
    """
    data = _http_get(f"{API_BASE_V3}/api/status")
    if "error" in data:
        return f"‚ùå –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {data['error']}\n\n–ü–µ—Ä–µ–≤—ñ—Ä: —á–∏ –∑–∞–ø—É—â–µ–Ω–æ `python -m app.main --mode all --stdio pipe`?"

    status = data.get("status", data)
    lines = ["# üü¢ Platform Status\n"]

    # Boot info
    boot_id = status.get("boot_id", "?")
    uptime = status.get("disk_bootstrap_elapsed_s", 0)
    prime = status.get("prime_ready", False)
    lines.append(f"**Boot ID**: `{boot_id[:12]}...`")
    lines.append(f"**Uptime**: {uptime:.0f}s ({uptime/3600:.1f}h)")
    lines.append(f"**Prime Ready**: {'‚úÖ Yes' if prime else '‚è≥ No'}")

    # Prime payload
    pp = status.get("prime_ready_payload", {})
    if isinstance(pp, dict):
        tails = pp.get("prime_tail_len_by_tf_s", {})
        if tails:
            lines.append("\n## Bar Counts per TF (Redis tail)")
            for tf_s, count in sorted(tails.items(), key=lambda x: int(x[0])):
                label = TF_LABELS.get(int(tf_s), tf_s)
                lines.append(f"  - **{label}** ({tf_s}s): {count} bars")

    # Redis
    redis_ok = status.get("redis_enabled", False)
    mismatch = status.get("redis_spec_mismatch", False)
    lines.append(f"\n**Redis**: {'‚úÖ Enabled' if redis_ok else '‚ùå Disabled'}")
    if mismatch:
        fields = status.get("redis_spec_mismatch_fields", [])
        lines.append(f"‚ö†Ô∏è **Redis spec mismatch**: {', '.join(fields)}")

    # Preview
    preview_tfs = status.get("preview_tf_allowlist_s", [])
    preview_labels = [TF_LABELS.get(t, str(t)) for t in preview_tfs]
    lines.append(f"**Preview TFs**: {', '.join(preview_labels)}")
    lines.append(f"**Preview updates total**: {status.get('preview_tail_updates_total', 0)}")

    # NoMix violation
    if status.get("preview_nomix_violation"):
        reason = status.get("preview_nomix_violation_reason", "?")
        lines.append(f"üö® **NoMix violation** (I3): {reason}")

    # Disk policy
    lines.append(f"\n**Disk hotpath blocked**: {status.get('disk_hotpath_blocked_total', 0)}")
    lines.append(f"**Disk bootstrap reads**: {status.get('disk_bootstrap_reads_total', 0)}")

    # Targets
    targets = status.get("prime_target_min_by_tf_s", {})
    if targets:
        lines.append("\n## Cold-start targets (min bars)")
        for tf_s, min_n in sorted(targets.items(), key=lambda x: int(x[0])):
            label = TF_LABELS.get(int(tf_s), tf_s)
            lines.append(f"  - {label}: {min_n}")

    lines.append(f"\n*Snapshot at {_format_ms(status.get('ts_ms', 0))}*")
    return "\n".join(lines)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# TOOL 2: Inspect Bars
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@mcp.tool()
def inspect_bars(
    symbol: str = "XAU/USD",
    tf: str = "M5",
    limit: int = 20,
    show_last: int = 5,
) -> str:
    """
    –û—Ç—Ä–∏–º–∞—Ç–∏ –±–∞—Ä–∏ –∑ HTTP API –ø–ª–∞—Ç—Ñ–æ—Ä–º–∏ —Ç–∞ –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ —ó—Ö.

    –ü–æ–∫–∞–∑—É—î: –æ—Å—Ç–∞–Ω–Ω—ñ N –±–∞—Ä—ñ–≤, gaps, complete/preview —Å—Ç–∞–Ω,
    –≤—ñ–∫ –æ—Å—Ç–∞–Ω–Ω—å–æ–≥–æ –±–∞—Ä—É, meta —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é.

    Args:
        symbol: –°–∏–º–≤–æ–ª (XAU/USD, NAS100, SPX500...)
        tf: TimeFrame ‚Äî label (M1, M5, H1, H4, D1) –∞–±–æ —á–∏—Å–ª–æ (300, 3600...)
        limit: –°–∫—ñ–ª—å–∫–∏ –±–∞—Ä—ñ–≤ –∑–∞–ø—Ä–æ—Å–∏—Ç–∏ –∑ API (max 30000)
        show_last: –°–∫—ñ–ª—å–∫–∏ –æ—Å—Ç–∞–Ω–Ω—ñ—Ö –ø–æ–∫–∞–∑–∞—Ç–∏ —É –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
    """
    tf_s = _tf_resolve(tf)
    if tf_s is None:
        return f"‚ùå –ù–µ–≤—ñ–¥–æ–º–∏–π TF: {tf}. –î–æ–ø—É—Å—Ç–∏–º—ñ: {', '.join(TF_LABELS.values())}"

    url = f"{API_BASE_V3}/api/bars?symbol={symbol}&tf_s={tf_s}&limit={limit}"
    data = _http_get(url)
    if "error" in data:
        return f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—å –æ—Ç—Ä–∏–º–∞—Ç–∏ –±–∞—Ä–∏: {data['error']}"

    if not data.get("ok"):
        return f"‚ùå API –ø–æ–≤–µ—Ä–Ω—É–≤ –ø–æ–º–∏–ª–∫—É: {json.dumps(data, ensure_ascii=False)[:500]}"

    bars = data.get("bars", [])
    meta = data.get("meta", {})
    warnings = data.get("warnings", [])
    boot_id = data.get("boot_id", "?")
    tf_label = TF_LABELS.get(tf_s, str(tf_s))

    lines = [f"# üìä Bars: {symbol} {tf_label}\n"]
    lines.append(f"**Total bars**: {len(bars)}")
    lines.append(f"**Boot ID**: `{boot_id[:12]}...`")
    lines.append(f"**Source**: {meta.get('source', '?')}")

    if meta.get("extensions"):
        ext = meta["extensions"]
        lines.append(f"**Plane**: {ext.get('plane', '?')}")

    if warnings:
        lines.append(f"\n‚ö†Ô∏è **Warnings**: {'; '.join(warnings)}")

    if not bars:
        lines.append("\n*–ù–µ–º–∞—î –±–∞—Ä—ñ–≤.*")
        return "\n".join(lines)

    # Date range
    first = bars[0]
    last = bars[-1]
    lines.append(f"\n**Range**: {_format_ms(first.get('open_time_ms', 0))} ‚Üí {_format_ms(last.get('open_time_ms', 0))}")

    # Age of last bar
    last_close_ms = last.get("close_time_ms", last.get("open_time_ms", 0) + tf_s * 1000)
    age_s = (time.time() * 1000 - last_close_ms) / 1000
    lines.append(f"**Last bar age**: {age_s:.0f}s ({age_s/60:.1f}min)")
    lines.append(f"**Last bar complete**: {last.get('complete', '?')}")

    # Gap analysis
    gaps = []
    for i in range(1, len(bars)):
        expected = bars[i - 1].get("open_time_ms", 0) + tf_s * 1000
        actual = bars[i].get("open_time_ms", 0)
        if actual != expected:
            gap_bars = (actual - expected) / (tf_s * 1000)
            gaps.append({
                "after": _format_ms(bars[i - 1].get("open_time_ms", 0)),
                "expected": _format_ms(expected),
                "actual": _format_ms(actual),
                "gap_bars": gap_bars,
            })

    if gaps:
        lines.append(f"\n## ‚ö†Ô∏è Gaps detected: {len(gaps)}")
        for g in gaps[:10]:  # max 10
            lines.append(f"  - After {g['after']}: gap of {g['gap_bars']:.0f} bars")
    else:
        lines.append(f"\n‚úÖ **No gaps** —É {len(bars)} –±–∞—Ä–∞—Ö")

    # Complete/preview stats
    complete_count = sum(1 for b in bars if b.get("complete"))
    preview_count = len(bars) - complete_count
    lines.append(f"\n**Complete**: {complete_count} | **Preview**: {preview_count}")

    # Source distribution
    sources: dict[str, int] = {}
    for b in bars:
        s = b.get("src", "unknown")
        sources[s] = sources.get(s, 0) + 1
    lines.append(f"**Sources**: {', '.join(f'{k}={v}' for k, v in sorted(sources.items()))}")

    # Last N bars
    show = min(show_last, len(bars))
    lines.append(f"\n## Last {show} bars")
    lines.append("| Time (UTC) | O | H | L | C | Vol | Complete | Src |")
    lines.append("|---|---|---|---|---|---|---|---|")
    for b in bars[-show:]:
        t = _format_ms(b.get("open_time_ms", 0))
        lines.append(
            f"| {t} | {b.get('open', 0):.2f} | {b.get('high', 0):.2f} | "
            f"{b.get('low', 0):.2f} | {b.get('close', 0):.2f} | "
            f"{b.get('volume', 0)} | {b.get('complete', '?')} | {b.get('src', '?')} |"
        )

    return "\n".join(lines)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# TOOL 3: Inspect Updates (live events)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@mcp.tool()
def inspect_updates(
    symbol: str = "XAU/USD",
    tf: str = "M5",
    limit: int = 10,
) -> str:
    """
    –û—Ç—Ä–∏–º–∞—Ç–∏ –æ—Å—Ç–∞–Ω–Ω—ñ live update events –∑ /api/updates.
    –ü–æ–∫–∞–∑—É—î: cursor_seq, boot_id, event_ts, complete/source,
    –∑–∞—Ç—Ä–∏–º–∫—É –º—ñ–∂ event —Ç–∞ API.

    Args:
        symbol: –°–∏–º–≤–æ–ª
        tf: TimeFrame (label –∞–±–æ —á–∏—Å–ª–æ)
        limit: –ö—ñ–ª—å–∫—ñ—Å—Ç—å events
    """
    tf_s = _tf_resolve(tf)
    if tf_s is None:
        return f"‚ùå –ù–µ–≤—ñ–¥–æ–º–∏–π TF: {tf}"

    url = f"{API_BASE_V3}/api/updates?symbol={symbol}&tf_s={tf_s}&limit={limit}"
    data = _http_get(url)
    if "error" in data:
        return f"‚ùå {data['error']}"
    if not data.get("ok"):
        return f"‚ùå API error: {json.dumps(data, ensure_ascii=False)[:400]}"

    events = data.get("events", [])
    cursor = data.get("cursor_seq", 0)
    boot_id = data.get("boot_id", "?")
    tf_label = TF_LABELS.get(tf_s, str(tf_s))

    lines = [f"# üîÑ Updates: {symbol} {tf_label}\n"]
    lines.append(f"**Cursor seq**: {cursor}")
    lines.append(f"**Boot ID**: `{boot_id[:12]}...`")
    lines.append(f"**Events count**: {len(events)}")

    # Latency
    api_ts = data.get("api_seen_ts_ms", 0)
    ssot_ts = data.get("ssot_write_ts_ms", 0)
    if api_ts and ssot_ts:
        latency = api_ts - ssot_ts
        lines.append(f"**API-to-SSOT latency**: {latency}ms")

    if not events:
        lines.append("\n*–ù–µ–º–∞—î –Ω–æ–≤–∏—Ö events.*")
        return "\n".join(lines)

    lines.append("\n## Events")
    for ev in events[-10:]:
        bar = ev.get("bar", {})
        key = ev.get("key", {})
        lines.append(
            f"- **{_format_ms(key.get('open_ms', 0))}** | "
            f"C={bar.get('close', 0):.2f} V={bar.get('volume', 0)} | "
            f"complete={ev.get('complete')} src={ev.get('source', '?')}"
        )

    return "\n".join(lines)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# TOOL 4: Platform Config
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@mcp.tool()
def platform_config(section: str = "") -> str:
    """
    –ü—Ä–æ—á–∏—Ç–∞—Ç–∏ config.json ‚Äî SSOT –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é –ø–ª–∞—Ç—Ñ–æ—Ä–º–∏.

    Args:
        section: –ö–æ–Ω–∫—Ä–µ—Ç–Ω–∞ —Å–µ–∫—Ü—ñ—è (redis, bootstrap, m1_poller, channels,
                 market_calendar_by_group, ws_server) –∞–±–æ –ø—É—Å—Ç–æ –¥–ª—è –ø–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ñ—ñ–≥—É
    """
    cfg = _load_config()
    if "error" in cfg:
        return f"‚ùå {cfg['error']}"

    if section:
        val = cfg.get(section)
        if val is None:
            available = ", ".join(k for k in cfg if isinstance(cfg[k], dict))
            return f"‚ùå –°–µ–∫—Ü—ñ—è '{section}' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞. –î–æ—Å—Ç—É–ø–Ω—ñ: {available}"
        return f"# Config: {section}\n```json\n{json.dumps(val, indent=2, ensure_ascii=False)}\n```"

    # Summary
    lines = ["# üìã Platform Config (SSOT)\n"]
    lines.append(f"**–°–∏–º–≤–æ–ª–∏**: {', '.join(cfg.get('symbols', []))}")

    tfa = cfg.get("tf_allowlist_s", [])
    labels = [TF_LABELS.get(t, str(t)) for t in tfa]
    lines.append(f"**TF allowlist**: {', '.join(labels)}")
    lines.append(f"**Broker base TFs**: {cfg.get('broker_base_tfs_s', [])} (–ø–æ—Ä–æ–∂–Ω—ñ–π = ADR-0023)")
    lines.append(f"**Derived TFs**: {[TF_LABELS.get(t, t) for t in cfg.get('derived_tfs_s', [])]}")

    ptf = cfg.get("preview_tick_tfs_s", [])
    lines.append(f"**Preview TFs**: {[TF_LABELS.get(t, str(t)) for t in ptf]}")
    lines.append(f"**Preview interval**: {cfg.get('preview_tick_publish_min_interval_ms', '?')}ms")

    lines.append(f"\n**Tick stream**: {'‚úÖ' if cfg.get('tick_stream_enabled') else '‚ùå'}")
    lines.append(f"**D1 tick relay**: {'‚úÖ' if cfg.get('d1_live_tick_relay_enabled') else '‚ùå'}")
    lines.append(f"**Calendar gate**: {'‚úÖ' if cfg.get('calendar_gate_enabled') else '‚ùå'}")

    # Redis
    rcfg = cfg.get("redis", {})
    lines.append(f"\n**Redis**: {'‚úÖ' if rcfg.get('enabled') else '‚ùå'} "
                 f"({rcfg.get('host', '?')}:{rcfg.get('port', '?')}/db{rcfg.get('db', '?')})")
    lines.append(f"**Redis namespace**: {rcfg.get('namespace', '?')}")

    # WS
    wcfg = cfg.get("ws_server", {})
    lines.append(f"\n**WS server**: {'‚úÖ' if wcfg.get('enabled') else '‚ùå'} "
                 f"({wcfg.get('host', '?')}:{wcfg.get('port', '?')})")

    # Anchors
    lines.append(f"\n**H4 anchor offset**: {cfg.get('day_anchor_offset_s', '?')}s "
                 f"({cfg.get('day_anchor_offset_s', 0) // 3600}:{(cfg.get('day_anchor_offset_s', 0) % 3600) // 60:02d} UTC)")
    lines.append(f"**D1 anchor offset**: {cfg.get('day_anchor_offset_s_d1', '?')}s "
                 f"({cfg.get('day_anchor_offset_s_d1', 0) // 3600}:{(cfg.get('day_anchor_offset_s_d1', 0) % 3600) // 60:02d} UTC)")

    # Calendar groups
    groups = cfg.get("market_calendar_symbol_groups", {})
    if groups:
        lines.append("\n## Symbol ‚Üí Calendar Group")
        for sym, grp in sorted(groups.items()):
            lines.append(f"  - {sym} ‚Üí `{grp}`")

    return "\n".join(lines)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# TOOL 5: Redis Inspect
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@mcp.tool()
def redis_inspect(
    pattern: str = "v3_local:*",
    command: str = "keys",
    key: str = "",
    limit: int = 50,
) -> str:
    """
    –Ü–Ω—Å–ø–µ–∫—Ç—É–≤–∞—Ç–∏ Redis live —Å—Ç–∞–Ω –ø–ª–∞—Ç—Ñ–æ—Ä–º–∏.

    Args:
        pattern: Key pattern –¥–ª—è SCAN (default: v3_local:*)
        command: –ö–æ–º–∞–Ω–¥–∞: 'keys' (—Å–ø–∏—Å–æ–∫ –∫–ª—é—á—ñ–≤), 'get' (–∑–Ω–∞—á–µ–Ω–Ω—è –∫–ª—é—á–∞),
                 'type' (—Ç–∏–ø –∫–ª—é—á–∞), 'ttl' (TTL –∫–ª—é—á–∞),
                 'dbsize' (–∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª—é—á—ñ–≤), 'info' (Redis info)
        key: –ö–æ–Ω–∫—Ä–µ—Ç–Ω–∏–π –∫–ª—é—á –¥–ª—è get/type/ttl
        limit: Max –∫–ª—é—á—ñ–≤ –¥–ª—è keys –∫–æ–º–∞–Ω–¥–∏
    """
    r = _redis_client()
    if r is None:
        return "‚ùå Redis –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π. –ü–µ—Ä–µ–≤—ñ—Ä: redis-server –∑–∞–ø—É—â–µ–Ω–∏–π? config.json redis.enabled=true?"

    try:
        if command == "dbsize":
            size = r.dbsize()
            return f"# Redis DB size\n**Keys**: {size}"

        if command == "info":
            info = r.info()
            lines = ["# Redis Info\n"]
            for k in ["redis_version", "used_memory_human", "connected_clients",
                       "uptime_in_seconds", "keyspace_hits", "keyspace_misses"]:
                lines.append(f"**{k}**: {info.get(k, '?')}")
            db_info = info.get(f"db{r.connection_pool.connection_kwargs.get('db', 0)}", {})
            if db_info:
                lines.append(f"\n**DB stats**: {db_info}")
            return "\n".join(lines)

        if command == "get":
            if not key:
                return "‚ùå –í–∫–∞–∂–∏ key –¥–ª—è –∫–æ–º–∞–Ω–¥–∏ 'get'"
            val = r.get(key)
            if val is None:
                # –°–ø—Ä–æ–±—É—î–º–æ —è–∫ hash
                t = r.type(key)
                if t == "hash":
                    data = r.hgetall(key)
                    return f"# Redis HASH: {key}\n```json\n{json.dumps(data, indent=2, ensure_ascii=False)}\n```"
                elif t == "list":
                    data = r.lrange(key, 0, limit - 1)
                    return f"# Redis LIST: {key} ({r.llen(key)} items)\n```json\n{json.dumps(data[:limit], indent=2)}\n```"
                elif t == "zset":
                    data = r.zrange(key, 0, limit - 1, withscores=True)
                    return f"# Redis ZSET: {key} ({r.zcard(key)} members)\n{data[:limit]}"
                return f"Key `{key}` –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ (type={t})"
            try:
                parsed = json.loads(val)
                return f"# Redis GET: {key}\n```json\n{json.dumps(parsed, indent=2, ensure_ascii=False)}\n```"
            except (json.JSONDecodeError, TypeError):
                return f"# Redis GET: {key}\n```\n{val[:2000]}\n```"

        if command == "type":
            if not key:
                return "‚ùå –í–∫–∞–∂–∏ key"
            return f"Type of `{key}`: **{r.type(key)}** (TTL: {r.ttl(key)}s)"

        if command == "ttl":
            if not key:
                return "‚ùå –í–∫–∞–∂–∏ key"
            return f"TTL of `{key}`: **{r.ttl(key)}**s"

        # Default: keys
        cursor = 0
        keys_list = []
        while len(keys_list) < limit:
            cursor, batch = r.scan(cursor, match=pattern, count=100)
            keys_list.extend(batch)
            if cursor == 0:
                break

        keys_list = sorted(keys_list[:limit])
        lines = [f"# Redis Keys: `{pattern}`\n"]
        lines.append(f"**Found**: {len(keys_list)} keys (limit={limit})\n")

        # Group by prefix
        groups: dict[str, list] = {}
        for k in keys_list:
            prefix = ":".join(k.split(":")[:3]) if ":" in k else k
            groups.setdefault(prefix, []).append(k)

        for prefix, kk in sorted(groups.items()):
            lines.append(f"### `{prefix}` ({len(kk)} keys)")
            for k in kk[:20]:
                t = r.type(k)
                ttl = r.ttl(k)
                lines.append(f"  - `{k}` (type={t}, ttl={ttl}s)")

        return "\n".join(lines)

    except Exception as e:
        return f"‚ùå Redis –ø–æ–º–∏–ª–∫–∞: {e}"


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# TOOL 6: Data Files Audit
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@mcp.tool()
def data_files_audit(
    symbol: str = "XAU/USD",
    tf: str = "M1",
    last_n_files: int = 3,
) -> str:
    """
    –ê—É–¥–∏—Ç JSONL data —Ñ–∞–π–ª—ñ–≤ –Ω–∞ –¥–∏—Å–∫—É (data_v3/).

    –ü–æ–∫–∞–∑—É—î: –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ñ–∞–π–ª—ñ–≤, –∑–∞–≥–∞–ª—å–Ω—É –∫—ñ–ª—å–∫—ñ—Å—Ç—å –±–∞—Ä—ñ–≤,
    date range, –æ—Å—Ç–∞–Ω–Ω—ñ —Ñ–∞–π–ª–∏ –∑ –¥–µ—Ç–∞–ª—è–º–∏.

    Args:
        symbol: –°–∏–º–≤–æ–ª
        tf: TimeFrame
        last_n_files: –°–∫—ñ–ª—å–∫–∏ –æ—Å—Ç–∞–Ω–Ω—ñ—Ö —Ñ–∞–π–ª—ñ–≤ –ø–æ–∫–∞–∑–∞—Ç–∏ –¥–µ—Ç–∞–ª—å–Ω–æ
    """
    tf_s = _tf_resolve(tf)
    if tf_s is None:
        return f"‚ùå –ù–µ–≤—ñ–¥–æ–º–∏–π TF: {tf}"

    sym_dir = _sym_dir(symbol)
    tf_dir = DATA_ROOT / sym_dir / f"tf_{tf_s}"

    if not tf_dir.exists():
        # –°–ø—Ä–æ–±—É—î–º–æ –∑–Ω–∞–π—Ç–∏
        candidates = list(DATA_ROOT.glob(f"**/tf_{tf_s}"))
        if candidates:
            return f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è {tf_dir} –Ω–µ —ñ—Å–Ω—É—î. –ó–Ω–∞–π–¥–µ–Ω–æ: {[str(c) for c in candidates]}"
        return f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è {tf_dir} –Ω–µ —ñ—Å–Ω—É—î. –ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è {symbol} {TF_LABELS.get(tf_s, tf)}"

    jsonl_files = sorted(tf_dir.glob("*.jsonl"))
    tf_label = TF_LABELS.get(tf_s, str(tf_s))

    lines = [f"# üíæ Data Files: {symbol} {tf_label}\n"]
    lines.append(f"**Directory**: `{tf_dir.relative_to(PROJECT_ROOT)}`")
    lines.append(f"**JSONL files**: {len(jsonl_files)}")

    if not jsonl_files:
        lines.append("\n*–ù–µ–º–∞—î —Ñ–∞–π–ª—ñ–≤.*")
        return "\n".join(lines)

    # Total size
    total_bytes = sum(f.stat().st_size for f in jsonl_files)
    lines.append(f"**Total size**: {total_bytes / 1024 / 1024:.2f} MB")
    lines.append(f"**Date range**: `{jsonl_files[0].stem}` ‚Üí `{jsonl_files[-1].stem}`")

    # Count total bars (sample first and last files)
    total_bars = 0
    for f in jsonl_files:
        try:
            with open(f, "r", encoding="utf-8") as fh:
                total_bars += sum(1 for line in fh if line.strip())
        except Exception:
            pass

    lines.append(f"**Total bars**: {total_bars:,}")

    # Last N files detail
    lines.append(f"\n## Last {last_n_files} files")
    for f in jsonl_files[-last_n_files:]:
        try:
            with open(f, "r", encoding="utf-8") as fh:
                file_lines = [low.strip() for low in fh if low.strip()]
            bar_count = len(file_lines)
            if file_lines:
                first_bar = json.loads(file_lines[0])
                last_bar = json.loads(file_lines[-1])
                lines.append(f"\n### `{f.name}` ({bar_count} bars)")
                lines.append(f"  - First: {_format_ms(first_bar.get('open_time_ms', 0))}")
                lines.append(f"  - Last: {_format_ms(last_bar.get('open_time_ms', 0))}")
                lines.append(f"  - Size: {f.stat().st_size / 1024:.1f} KB")

                # Check monotonicity
                timestamps = [json.loads(low).get("open_time_ms", 0) for low in file_lines]
                monotonic = all(timestamps[i] < timestamps[i + 1] for i in range(len(timestamps) - 1))
                lines.append(f"  - Monotonic: {'‚úÖ' if monotonic else '‚ùå –ü–û–†–£–®–ï–ù–ù–Ø!'}")
            else:
                lines.append(f"\n### `{f.name}` (–ø–æ—Ä–æ–∂–Ω—ñ–π)")
        except Exception as e:
            lines.append(f"\n### `{f.name}` ‚Äî –ø–æ–º–∏–ª–∫–∞: {e}")

    return "\n".join(lines)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# TOOL 7: Run Exit Gates
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@mcp.tool()
def run_exit_gates(gate_name: str = "") -> str:
    """
    –ó–∞–ø—É—Å—Ç–∏—Ç–∏ exit gates ‚Äî quality gates –ø–ª–∞—Ç—Ñ–æ—Ä–º–∏.
    –ü–µ—Ä–µ–≤—ñ—Ä—è—é—Ç—å dependency rule, HTF availability, D1 anchor alignment.

    Args:
        gate_name: –ö–æ–Ω–∫—Ä–µ—Ç–Ω–∏–π gate (dependency_rule, htf_available,
                   d1_anchor_alignment) –∞–±–æ –ø—É—Å—Ç–æ –¥–ª—è ALL
    """
    python = str(PROJECT_ROOT / ".venv" / "Scripts" / "python.exe")
    if not os.path.exists(python):
        python = sys.executable

    try:
        cmd = [python, "-m", "tools.run_exit_gates"]
        if gate_name:
            cmd.extend(["--gate", gate_name])
        else:
            cmd.extend(["--manifest", "tools/exit_gates/manifest.json"])

        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=30,
            cwd=str(PROJECT_ROOT),
        )

        output = result.stdout + result.stderr
        if result.returncode == 0:
            return f"# ‚úÖ Exit Gates\n\n```\n{output.strip()}\n```"
        else:
            return f"# ‚ùå Exit Gates FAILED (code={result.returncode})\n\n```\n{output.strip()}\n```"
    except subprocess.TimeoutExpired:
        return "‚ùå Exit gates timeout (>30s)"
    except Exception as e:
        return f"‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç–∏ exit gates: {e}"


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# TOOL 8: Log Tail
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@mcp.tool()
def log_tail(
    process: str = "",
    lines_count: int = 50,
    grep: str = "",
) -> str:
    """
    –ü—Ä–æ—á–∏—Ç–∞—Ç–∏ –æ—Å—Ç–∞–Ω–Ω—ñ —Ä—è–¥–∫–∏ –ª–æ–≥—ñ–≤ –ø—Ä–æ—Ü–µ—Å—É.

    Args:
        process: –Ü–º'—è –ø—Ä–æ—Ü–µ—Å—É (m1_poller, tick_publisher, tick_preview,
                 connector, ui, ws_server) –∞–±–æ –ø—É—Å—Ç–æ –¥–ª—è —Å–ø–∏—Å–∫—É –¥–æ—Å—Ç—É–ø–Ω–∏—Ö
        lines_count: –ö—ñ–ª—å–∫—ñ—Å—Ç—å –æ—Å—Ç–∞–Ω–Ω—ñ—Ö —Ä—è–¥–∫—ñ–≤
        grep: –§—ñ–ª—å—Ç—Ä (–ø—ñ–¥—Ä—è–¥–æ–∫ –¥–ª—è –ø–æ—à—É–∫—É –≤ –ª–æ–≥–∞—Ö)
    """
    if not LOGS_DIR.exists():
        return "‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è logs/ –Ω–µ —ñ—Å–Ω—É—î"

    log_files = sorted(LOGS_DIR.glob("*.log"))
    log_files += sorted(LOGS_DIR.glob("*.out.log"))

    if not process:
        lines = ["# üìù Available Log Files\n"]
        for f in log_files:
            size = f.stat().st_size
            mtime = datetime.datetime.fromtimestamp(f.stat().st_mtime)
            lines.append(f"  - `{f.name}` ({size / 1024:.1f} KB, modified {mtime:%Y-%m-%d %H:%M})")
        if not log_files:
            lines.append("*–ù–µ–º–∞—î log —Ñ–∞–π–ª—ñ–≤.*")
        return "\n".join(lines)

    # Find matching log
    matches = [f for f in log_files if process in f.stem]
    if not matches:
        # Also check stdout/stderr pattern
        matches = list(LOGS_DIR.glob(f"{process}*.log"))
        matches += list(LOGS_DIR.glob(f"*{process}*.log"))

    if not matches:
        available = ", ".join(f.stem for f in log_files)
        return f"‚ùå –õ–æ–≥ –¥–ª—è '{process}' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. –î–æ—Å—Ç—É–ø–Ω—ñ: {available}"

    log_file = matches[0]
    try:
        with open(log_file, "r", encoding="utf-8", errors="replace") as fh:
            all_lines = fh.readlines()

        if grep:
            all_lines = [low for low in all_lines if grep.lower() in low.lower()]

        tail = all_lines[-lines_count:]
        header = f"# üìù Log: {log_file.name}"
        if grep:
            header += f" (filtered: '{grep}')"
        header += f"\n*{len(all_lines)} total lines, showing last {len(tail)}*\n"

        return header + "```\n" + "".join(tail) + "```"
    except Exception as e:
        return f"‚ùå –ü–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è {log_file}: {e}"


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# TOOL 9: Derive Chain Status
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@mcp.tool()
def derive_chain_status(symbol: str = "XAU/USD") -> str:
    """
    –ü–æ–∫–∞–∑–∞—Ç–∏ —Å—Ç–∞–Ω derive chain –¥–ª—è —Å–∏–º–≤–æ–ª—É.
    –ü–µ—Ä–µ–≤—ñ—Ä—è—î –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –¥–∞–Ω–∏—Ö –ø–æ –≤—Å—ñ—Ö TF –∫–∞—Å–∫–∞–¥—É M1‚ÜíD1.

    Args:
        symbol: –°–∏–º–≤–æ–ª
    """
    cfg = _load_config()
    tf_allowlist = cfg.get("tf_allowlist_s", [60, 180, 300, 900, 1800, 3600, 14400, 86400])
    sym_dir = _sym_dir(symbol)

    lines = [f"# üîó Derive Chain: {symbol}\n"]
    lines.append("–ö–∞—Å–∫–∞–¥: `M1‚ÜíM3(√ó3)‚ÜíM5(√ó5)‚ÜíM15(√ó3)‚ÜíM30(√ó2)‚ÜíH1(√ó2)‚ÜíH4(√ó4)` + `M1‚ÜíD1(√ó1440)`\n")

    chain = [
        (60, "M1", "broker/poller"),
        (180, "M3", "derived from M1√ó3"),
        (300, "M5", "derived from M1√ó5"),
        (900, "M15", "derived from M5√ó3"),
        (1800, "M30", "derived from M15√ó2"),
        (3600, "H1", "derived from M30√ó2"),
        (14400, "H4", "derived from H1√ó4"),
        (86400, "D1", "derived from M1√ó1440"),
    ]

    lines.append("| TF | Source | Files | Bars | Last Bar | Age |")
    lines.append("|---|---|---|---|---|---|")

    for tf_s, label, source in chain:
        tf_dir = DATA_ROOT / sym_dir / f"tf_{tf_s}"
        if not tf_dir.exists():
            lines.append(f"| **{label}** | {source} | ‚ùå missing | 0 | ‚Äî | ‚Äî |")
            continue

        jsonl_files = sorted(tf_dir.glob("*.jsonl"))
        total_bars = 0
        last_bar_ms = 0

        for f in jsonl_files:
            try:
                with open(f, "r", encoding="utf-8") as fh:
                    for line in fh:
                        if line.strip():
                            total_bars += 1
                            try:
                                b = json.loads(line)
                                ms = b.get("open_time_ms", 0)
                                if ms > last_bar_ms:
                                    last_bar_ms = ms
                            except json.JSONDecodeError:
                                pass
            except Exception:
                pass

        if last_bar_ms:
            age_s = (time.time() * 1000 - last_bar_ms) / 1000
            age_str = f"{age_s / 3600:.1f}h" if age_s > 3600 else f"{age_s / 60:.0f}m"
            lines.append(
                f"| **{label}** | {source} | {len(jsonl_files)} | {total_bars:,} | "
                f"{_format_ms(last_bar_ms)} | {age_str} |"
            )
        else:
            lines.append(f"| **{label}** | {source} | {len(jsonl_files)} | {total_bars} | ‚Äî | ‚Äî |")

    # Check Redis tail counts
    r = _redis_client()
    if r:
        lines.append("\n## Redis Tail Counts")
        ns = cfg.get("redis", {}).get("namespace", "v3_local")
        for tf_s, label, _ in chain:
            key = f"{ns}:ohlcv:snap:{_sym_dir(symbol)}:{tf_s}"
            try:
                t = r.type(key)
                if t == "list":
                    count = r.llen(key)
                elif t == "zset":
                    count = r.zcard(key)
                else:
                    count = "N/A"
                ttl = r.ttl(key)
                lines.append(f"  - **{label}**: {count} (type={t}, ttl={ttl}s)")
            except Exception:
                lines.append(f"  - **{label}**: ‚ùå error")

    return "\n".join(lines)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# TOOL 10: WS Server Status
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@mcp.tool()
def ws_server_check() -> str:
    """
    –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —Å—Ç–∞–Ω WebSocket —Å–µ—Ä–≤–µ—Ä–∞ (–ø–æ—Ä—Ç 8000).
    –ü–æ–∫–∞–∑—É—î: —á–∏ –∑–∞–ø—É—â–µ–Ω–∏–π, –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é, –ø—ñ–¥–∫–ª—é—á–µ–Ω–∏—Ö –∫–ª—ñ—î–Ω—Ç—ñ–≤.
    """
    cfg = _load_config()
    ws_cfg = cfg.get("ws_server", {})
    enabled = ws_cfg.get("enabled", False)
    host = ws_cfg.get("host", "127.0.0.1")
    port = ws_cfg.get("port", 8000)

    lines = [f"# üåê WS Server Status\n"]
    lines.append(f"**Config enabled**: {'‚úÖ' if enabled else '‚ùå'}")
    lines.append(f"**Endpoint**: ws://{host}:{port}/ws")
    lines.append(f"**Heartbeat**: {ws_cfg.get('heartbeat_interval_s', 30)}s")
    lines.append(f"**Delta poll**: {ws_cfg.get('delta_poll_interval_s', 1.0)}s")

    # Try HTTP root
    data = _http_get(f"http://{host}:{port}/", timeout=2)
    if "error" in data:
        lines.append(f"\n‚ö†Ô∏è –°–µ—Ä–≤–µ—Ä –Ω–µ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î –Ω–∞ HTTP: {data['error']}")
    else:
        lines.append(f"\n‚úÖ –°–µ—Ä–≤–µ—Ä –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î")

    return "\n".join(lines)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# TOOL 11: Quick Health Check
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@mcp.tool()
def health_check() -> str:
    """
    –®–≤–∏–¥–∫–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤'—è –≤—Å—ñ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤.
    –û–¥–∏–Ω –≤–∏–∫–ª–∏–∫ = –ø–æ–≤–Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∞: API, WS, Redis, Data, Exit Gates.
    """
    lines = ["# üè• Quick Health Check\n"]

    # 1. HTTP API
    status = _http_get(f"{API_BASE_V3}/api/status", timeout=3)
    if "error" in status:
        lines.append("## ‚ùå HTTP API (port 8089): OFFLINE")
        lines.append(f"  {status['error']}")
    else:
        s = status.get("status", status)
        prime = s.get("prime_ready", False)
        boot = s.get("boot_id", "?")[:12]
        uptime = s.get("disk_bootstrap_elapsed_s", 0)
        lines.append(f"## ‚úÖ HTTP API (port 8089): OK")
        lines.append(f"  Boot: `{boot}...` | Prime: {'‚úÖ' if prime else '‚è≥'} | Uptime: {uptime:.0f}s")

    # 2. WS Server
    cfg = _load_config()
    ws_port = cfg.get("ws_server", {}).get("port", 8000)
    ws = _http_get(f"http://127.0.0.1:{ws_port}/", timeout=2)
    if "error" in ws:
        lines.append(f"\n## ‚ö†Ô∏è WS Server (port {ws_port}): NOT RESPONDING")
    else:
        lines.append(f"\n## ‚úÖ WS Server (port {ws_port}): OK")

    # 3. Redis
    r = _redis_client()
    if r:
        try:
            r.ping()
            dbsize = r.dbsize()
            lines.append(f"\n## ‚úÖ Redis: OK ({dbsize} keys)")
        except Exception as e:
            lines.append(f"\n## ‚ùå Redis: ERROR ({e})")
    else:
        lines.append("\n## ‚ö†Ô∏è Redis: DISABLED or UNAVAILABLE")

    # 4. Data files
    symbols = cfg.get("symbols", [])
    for sym in symbols[:3]:  # max 3 symbols
        sym_path = DATA_ROOT / _sym_dir(sym)
        if sym_path.exists():
            tf_dirs = sorted(sym_path.glob("tf_*"))
            lines.append(f"\n## üíæ Data: {sym} ({len(tf_dirs)} TF dirs)")
            for td in tf_dirs:
                jsonl_count = len(list(td.glob("*.jsonl")))
                lines.append(f"  - `{td.name}`: {jsonl_count} files")
        else:
            lines.append(f"\n## ‚ö†Ô∏è Data: {sym} ‚Äî missing directory")

    # 5. Exit gates (quick: just dependency rule)
    python = str(PROJECT_ROOT / ".venv" / "Scripts" / "python.exe")
    if os.path.exists(python):
        try:
            result = subprocess.run(
                [python, "-c",
                 "import sys; sys.path.insert(0,'.'); "
                 "from tools.exit_gates.gates.gate_dependency_rule import run_gate; "
                 "r = run_gate({'root':'.'}); "
                 "print('PASS' if r['ok'] else 'FAIL: ' + r.get('details','?')[:100])"],
                capture_output=True, text=True, timeout=10,
                cwd=str(PROJECT_ROOT),
            )
            gate_result = result.stdout.strip()
            if "PASS" in gate_result:
                lines.append(f"\n## ‚úÖ Dependency Rule Gate: PASS")
            else:
                lines.append(f"\n## ‚ùå Dependency Rule Gate: {gate_result}")
        except Exception as e:
            lines.append(f"\n## ‚ö†Ô∏è Exit Gates: couldn't run ({e})")

    lines.append(f"\n*Check at {datetime.datetime.utcnow():%Y-%m-%d %H:%M:%S} UTC*")
    return "\n".join(lines)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# ENTRYPOINT
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

if __name__ == "__main__":
    mcp.run(transport="stdio")
